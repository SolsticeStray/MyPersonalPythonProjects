import requests
from bs4 import BeautifulSoup

# 1. å‡†å¤‡å·¥ä½œï¼šè®¾ç½® URL å’Œ ä¼ªè£…å¤´ (Wiki å¯¹æ²¡æœ‰ User-Agent çš„è¯·æ±‚å¾ˆæ•æ„Ÿ)
url = "https://zh.wikipedia.org/wiki/PewDiePieä¸T-Seriesä¹‹äº‰"
headers = {
    'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/91.0.4472.124 Safari/537.36'
}

print(f"æ­£åœ¨è®¿é—®: {url} ...")
response = requests.get(url, headers=headers)

if response.status_code == 200:
    # 2. æ¶ˆåŒ–ï¼šè§£æ HTML
    soup = BeautifulSoup(response.text, 'html.parser')

    # --- æ¨¡å— A: æŠ“å–æ ‡é¢˜å’Œç®€ä»‹ (æ–‡æœ¬) ---
    print("\n=== ğŸ“ é¡µé¢ç®€ä»‹ ===")
    title = soup.find('h1', id='firstHeading').text
    print(f"æ ‡é¢˜: {title}")

    # ç»´åŸºç™¾ç§‘çš„æ­£æ–‡é€šå¸¸åœ¨ mw-content-text -> mw-parser-output ä¸‹
    # æˆ‘ä»¬æŠ“å–å‰ä¸¤æ®µæ–‡å­—
    content_div = soup.find('div', class_='mw-parser-output')
    paragraphs = content_div.find_all('p', recursive=False)  # recursive=False é¿å…æŠ“åˆ°è¡¨æ ¼é‡Œçš„ p

    for i, p in enumerate(paragraphs[:2]):
        text = p.get_text().strip()
        if text:
            print(f"æ®µè½ {i + 1}: {text[:100]}...")  # åªæ˜¾ç¤ºå‰100ä¸ªå­—

    # --- æ¨¡å— B: æŠ“å–ä¾§è¾¹æ å›¾ç‰‡ (å›¾ç‰‡) ---
    print("\n=== ğŸ–¼ï¸ å…³é”®å›¾ç‰‡ ===")
    # ç»´åŸºç™¾ç§‘çš„ä¾§è¾¹æ é€šå¸¸æ˜¯ class="infobox"
    infobox = soup.find('table', class_='infobox')
    if infobox:
        img_tag = infobox.find('img')
        if img_tag:
            # ç»´åŸºç™¾ç§‘çš„å›¾ç‰‡é“¾æ¥é€šå¸¸æ˜¯ //upload.wikimedia... å¼€å¤´ï¼Œç¼ºå°‘ https:
            src = img_tag.get('src')
            if src.startswith('//'):
                src = 'https:' + src
            print(f"å‘ç°ä¸»å›¾é“¾æ¥: {src}")
            print("(ä½ å¯ä»¥å¤åˆ¶è¿™ä¸ªé“¾æ¥åœ¨æµè§ˆå™¨æ‰“å¼€ï¼Œæˆ–è€…ç”¨ä»£ç ä¸‹è½½å®ƒ)")
        else:
            print("Infobox ä¸­æœªæ‰¾åˆ°å›¾ç‰‡")

    # --- æ¨¡å— C: æŠ“å–ç»Ÿè®¡è¡¨æ ¼ (è¡¨æ ¼) ---
    print("\n=== ğŸ“Š æˆ˜å†µç»Ÿè®¡è¡¨æ ¼ ===")
    # ç»´åŸºç™¾ç§‘çš„æ ‡å‡†è¡¨æ ¼ class æ˜¯ "wikitable"
    # æˆ‘ä»¬å°è¯•æ‰¾åŒ…å«â€œæ—¶é—´â€å’Œâ€œæ¬¡æ•°â€çš„é‚£ä¸ªè¡¨æ ¼
    tables = soup.find_all('table', class_='wikitable')

    target_table = None
    for t in tables:
        # ç®€å•çš„åˆ¤æ–­ï¼šå¦‚æœè¡¨å¤´åŒ…å« "æ—¥æœŸ" æˆ– "æ—¶é—´"ï¼Œå¯èƒ½å°±æ˜¯æˆ‘ä»¬è¦æ‰¾çš„
        if "æ—¥æœŸ" in t.text or "æ™‚é–“" in t.text:
            target_table = t
            break

    if target_table:
        # éå†è¡¨æ ¼çš„è¡Œ (tr)
        rows = target_table.find_all('tr')
        print(f"æ‰¾åˆ°è¡¨æ ¼ï¼Œå…± {len(rows)} è¡Œï¼Œæ˜¾ç¤ºå‰ 5 è¡Œæ•°æ®ï¼š")

        for row in rows[:5]:  # åªæ¼”ç¤ºå‰5è¡Œ
            # æå–æ¯ä¸€è¡Œä¸­çš„å•å…ƒæ ¼ (th æˆ– td)
            cols = row.find_all(['th', 'td'])
            # ä½¿ç”¨åˆ—è¡¨æ¨å¯¼å¼æ¸…æ´—æ•°æ®ï¼šå»é™¤æ¢è¡Œç¬¦
            cols_text = [ele.text.strip() for ele in cols]
            print(cols_text)
    else:
        print("æœªæ‰¾åˆ°ç›®æ ‡ç»Ÿè®¡è¡¨æ ¼")

else:
    print("è®¿é—®å¤±è´¥ï¼Œè¯·æ£€æŸ¥ç½‘ç»œæˆ– URL")